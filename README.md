# One Permutation Variable Importance

An unofficial vibe-coded Python implementation of **"One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing"** ([arXiv:2512.13892v2](https://arxiv.org/abs/2512.13892)).

This package provides deterministic, efficient variable importance methods that are **10-100Ã— faster** than traditional permutation-based approaches while achieving comparable or better accuracy.

## ğŸ¯ Key Features

- **Direct Variable Importance (DVI)**: Single deterministic permutation replaces multiple random permutations
- **Systemic Variable Importance (SVI)**: Correlation-based propagation for fairness auditing and stress-testing
- **Deterministic**: Zero variance, perfectly reproducible results
- **Fast**: 10-100Ã— speedup over scikit-learn's baseline
- **Model-agnostic**: Works with any model having `predict()` or `predict_proba()`
- **Well-documented**: Comprehensive examples and API documentation

## ğŸ“¦ Installation

This project uses **uv** for dependency management:

```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies
uv pip install -r requirements.txt

# Or use standard pip
pip install -r requirements.txt

# The package is ready to use
cd permutation_vi
```

**Dependencies**: numpy, scipy, scikit-learn, pandas, matplotlib, seaborn
**Optional**: jupyter, ipykernel (for notebooks), trust-free (for TRUST model)

## ğŸš€ Quick Start

### Direct Variable Importance

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
from permutation_vi import DirectVariableImportance

# Generate data and train model
X, y = make_regression(n_samples=1000, n_features=10, n_informative=5)
model = RandomForestRegressor(random_state=42).fit(X, y)

# Compute importance with single optimal permutation
dvi = DirectVariableImportance(
    permutation_type='optimal',  # or 'approximate'
    scoring_metric='mae'         # or 'mse', 'rmse'
)
importance_scores = dvi.fit(model, X)

# View top features
top_features = dvi.get_top_features(n=5)
for idx, score in top_features:
    print(f"Feature {idx}: {score:.4f}")
```

### Systemic Variable Importance (Fairness Auditing)

```python
from permutation_vi import SystemicVariableImportance

# Detect indirect reliance on protected attributes
svi = SystemicVariableImportance(
    correlation_method='spearman',
    alpha=0.01  # Significance level for correlations
)

# Get decomposition: systemic = direct + indirect
systemic, direct, indirect = svi.fit(model, X, return_decomposition=True)

# Check if a feature has indirect importance (proxy effects)
feature_idx = 0
print(f"Direct importance:   {direct[feature_idx]:.4f}")
print(f"Indirect importance: {indirect[feature_idx]:.4f}")
print(f"Systemic importance: {systemic[feature_idx]:.4f}")

# Find proxy features
proxies = svi.get_feature_proxies(feature_idx)
print(f"Feature {feature_idx} has {len(proxies)} significant proxies")
```

## ğŸ“‹ Case Study Implementation Features

The complete case study implementations include:

### Experiment Orchestration
- **Master model training**: 10-fold cross-validation with row randomization (paper protocol)
- **Direct VI comparison**: Four methods (DVI-Optimal, DVI-Approximate, Breiman B=1, Breiman B=10)
- **Ground truth computation**: Random Forest importance averaged over 100 runs
- **Stability testing**: Track top-5 features across 10 runs with different random seeds
- **SVI fairness scenarios**: Multiple configurations testing protected attribute reliance

### Models Implemented
- **TRUST** (or RelaxedLasso approximation): Sparse linear with feature selection
- **OLS/Logistic**: Unregularized baselines
- **â„“1-Logistic**: Sparse classification with L1 penalty
- **Random Forest**: Ensemble baseline for ground truth

### Outputs Generated
- **Tables**: Formatted CSV files matching paper Tables 5-10, 15-20
- **Figures**: Correlation heatmaps (Figures 1-2) with Spearman correlation and significance thresholds
- **Metrics**: RÂ², MAE, PR-AUC, ROC-AUC, ground-truth correlation, speedup ratios
- **SVI decomposition**: Direct, indirect, and systemic importance for fairness auditing

### CLI Options
```bash
# Customize experiments
uv run python permutation_vi/case_studies/run_boston_hmda.py \
  --data-path data/boston_hmda/hmda_data.csv \
  --output-dir custom_results/ \
  --random-state 42 \
  --experiments master dvi stability svi heatmap
```

## ğŸ“Š Examples

Run the included examples to see the implementation in action:

```bash
# Basic examples (synthetic data)
uv run python permutation_vi/examples/example_dvi.py              # Direct Variable Importance
uv run python permutation_vi/examples/example_svi.py              # Systemic Importance
uv run python permutation_vi/examples/example_comparison.py       # Benchmark vs sklearn
uv run python permutation_vi/examples/example_benchmark.py        # Synthetic benchmarks

# Case studies (requires datasets)
uv run python permutation_vi/case_studies/run_boston_hmda.py      # Boston HMDA (Section 5.2)
uv run python permutation_vi/case_studies/run_german_credit.py    # German Credit (Section 5.3)
uv run python permutation_vi/case_studies/run_all_case_studies.py # Both case studies

# Interactive notebooks
jupyter notebook notebooks/case_studies_tutorial.ipynb            # Tutorial notebook
```

## ğŸ”¬ Reproducing Paper Results

This implementation includes the full test suite from the paper for validation and reproducibility.

### Synthetic Benchmark Suite (Section 3.2)

The paper tests across **192 scenarios** varying:
- Sample sizes: n âˆˆ {100, 1000, 10000}
- Dimensionality: p âˆˆ {10, 100}
- Noise levels: Ïƒ_Îµ âˆˆ {0.1, 5}
- Feature correlations: Ï âˆˆ {0, 0.3}
- Response types: Linear, Friedman function
- Task types: Regression, classification

**Run quick validation** (8 scenarios, 5 repetitions):
```python
from permutation_vi.benchmarks import create_scenario_grid, run_single_scenario, aggregate_results

# Get subset of scenarios
scenarios = create_scenario_grid()
test_scenarios = [s for s in scenarios if s['n'] == 1000 and s['p'] == 10]

# Run benchmarks
results = [run_single_scenario(s, n_repetitions=5) for s in test_scenarios]

# Aggregate results
summary = aggregate_results(results)
print(summary)
```

**Full benchmark** (192 scenarios, 50 repetitions - several hours runtime):
```bash
uv run python permutation_vi/examples/example_benchmark.py --full
```

**Expected results** (from paper Tables 1-4):
- Ground-truth correlation: >0.95
- Speedup: 1.7-2Ã— for small n, 10-100Ã— for large n
- Zero variance (perfectly deterministic)

### Real-World Case Studies (Sections 5.2 & 5.3)

Complete implementations reproducing all paper results including tables, figures, and fairness testing scenarios.

#### Boston HMDA Dataset (Section 5.2)

Debt-to-income ratio estimation with fairness testing:
- **Dataset**: 2380 observations, 12 features
- **Task**: Predict debt-to-income ratio
- **Protected attribute**: "black"
- **Master models**: TRUST (sparse linear), OLS, Random Forest

**Results reproduced**:
- âœ… Table 5: Master model performance (RÂ², MAE, nonzero coefficients)
- âœ… Tables 6-7: Direct VI comparisons (ground-truth correlation, speedup, max/mean score diff)
- âœ… Tables 15-16: Results with default MAE scoring (Appendix A)
- âœ… Table 19: Stability test showing deterministic DVI vs variable Breiman (Appendix B)
- âœ… Figure 1: Spearman correlation heatmap with threshold Î±=0.01
- âœ… SVI fairness testing: OLS scenario (0.44% systemic via proxies) and TRUST safeguarding (0.16% residual)

**Key findings**:
- Systemic importance of "black" is ~5Ã— its direct importance (via proxy features: lvr, deny, ccs, condominium)
- DVI achieves >0.94 correlation with ground truth
- 14-105Ã— faster than sklearn baseline
- Zero variance (perfectly deterministic) across runs

**Download dataset**:
```bash
# Place in data/boston_hmda/hmda_data.csv
# Source: https://github.com/adc-trust-ai/trust-free
```

**Run case study**:
```bash
# Single case study
uv run python permutation_vi/case_studies/run_boston_hmda.py --data-path data/boston_hmda/

# Results saved to: results/boston_hmda/tables/ and results/boston_hmda/figures/
```

#### German Credit Dataset (Section 5.3)

Credit risk classification with fairness testing:
- **Dataset**: 1000 observations, 20 features (7 numerical, 13 categorical)
- **Task**: Binary classification (credit risk prediction)
- **Protected attribute**: "Sex-Marital_status"
- **Master models**: â„“1-Logistic, Logistic (unregularized), Random Forest

**Results reproduced**:
- âœ… Table 8: Master model performance (PR-AUC, ROC-AUC, nonzero coefficients)
- âœ… Tables 9-10: Direct VI comparisons with sparse and unregularized logistic
- âœ… Tables 17-18: Results with default scoring (Appendix A)
- âœ… Table 20: Stability test (Appendix B)
- âœ… Figure 2: Correlation heatmap for categorical features
- âœ… SVI fairness testing: Unregularized logistic (3.24% systemic) vs sparse without protected (0% - safeguarding works)

**Key findings**:
- Systemic importance of "Sex-Marital_status": 3.24% total (2.09% direct + 1.15% indirect)
- Safeguarding by exclusion: Sparse model without protected attribute has 0% systemic importance
- Perfect stability: DVI returns identical top-5 features across 10 runs
- Sklearn Breiman shows 9-10 distinct top-5 rankings across runs

**Download dataset**:
```bash
# Place in data/german_credit/german_credit.csv
# Sources: UCI ML Repository or https://github.com/adc-trust-ai/trust-free
```

**Run case study**:
```bash
# Single case study
uv run python permutation_vi/case_studies/run_german_credit.py --data-path data/german_credit/

# Results saved to: results/german_credit/tables/ and results/german_credit/figures/
```

#### Run All Case Studies

Execute both case studies sequentially with a single command:
```bash
uv run python permutation_vi/case_studies/run_all_case_studies.py

# Options:
# --output-dir custom_results/     # Change output directory
# --skip-boston                     # Skip Boston HMDA
# --skip-german                     # Skip German Credit
# --random-state 42                 # Set random seed
```

#### Interactive Jupyter Notebooks

Explore the case studies interactively with step-by-step tutorials:

```bash
# Install Jupyter
uv pip install jupyter ipykernel

# Launch notebooks
jupyter notebook notebooks/

# Open: case_studies_tutorial.ipynb
```

**Notebook features**:
- ğŸ“Š Complete walkthrough of both case studies
- ğŸ¯ Reproduce Tables 5-10, Figures 1-2, and all SVI scenarios
- ğŸ“ˆ Interactive visualizations (correlation heatmaps, importance comparisons)
- ğŸ”¬ Educational explanations linking code to paper sections
- âš™ï¸ Configurable experiments (change models, parameters, features)

See [notebooks/README.md](notebooks/README.md) for detailed instructions.

## ğŸ§ª What the Paper Proposes

### The Problem with Traditional Permutation Importance

Traditional Breiman-style permutation importance:
1. Randomly permutes each feature B times (typically B=10)
2. Averages the results
3. Has non-zero variance â†’ stochastic instability
4. Requires computational overhead (B permutations per feature)

### The Solution: One Optimal Permutation

**Key Insight**: Replace B random permutations with a single deterministic optimal permutation.

**Optimal Permutation**: Cyclic shift by âŒŠn/2âŒ‹
- **Rank-shift** (optimal): O(n log n) due to sorting
- **Index-shift** (approximate): O(n) for speed

**Theoretical Guarantee** (Proposition 1):
- Maximizes the minimum circular displacement
- Achieves max-min optimality: m(Ï€) = âŒŠn/2âŒ‹

**Practical Benefits** (Proposition 2):
- Lower MSE when: |bias_difference| < Ïƒ/âˆšB
- Zero variance (deterministic)
- 10-100Ã— faster

### Extension: Systemic Variable Importance

**Purpose**: Detect indirect reliance on features through correlations

**Use Cases**:
1. **Fairness Auditing**: Find if a model relies on protected attributes via proxies
2. **Stress Testing**: Understand how shocks propagate through correlated features

**Method**:
1. Compute correlation threshold via permutation testing (FWER control)
2. Build correlation network (significant correlations only)
3. Propagate perturbations: x'_k = x_k + Î£_j cor(k,j) Ã— (x'_j - x_j)
4. Decompose: systemic = direct + indirect

## ğŸ“ Project Structure

```
permutation_vi/
â”œâ”€â”€ __init__.py                      # Package exports
â”œâ”€â”€ direct_importance.py             # DirectVariableImportance class
â”œâ”€â”€ systemic_importance.py           # SystemicVariableImportance class
â”œâ”€â”€ utils.py                         # Helper functions (Friedman, RF ground truth)
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ __init__.py                  # Benchmark exports
â”‚   â”œâ”€â”€ synthetic_suite.py           # 192-scenario test suite
â”‚   â””â”€â”€ reporting.py                 # Visualization and tables
â”œâ”€â”€ case_studies/
â”‚   â”œâ”€â”€ __init__.py                  # Case study exports
â”‚   â”œâ”€â”€ data_loaders.py              # Dataset loaders (Boston HMDA, German Credit)
â”‚   â”œâ”€â”€ models.py                    # Model factory (TRUST/RelaxedLasso, logistic, RF)
â”‚   â”œâ”€â”€ cv_utils.py                  # Cross-validation utilities
â”‚   â”œâ”€â”€ metrics.py                   # Metric computations (PR-AUC, ROC-AUC, Brier)
â”‚   â”œâ”€â”€ table_generators.py          # Format output tables (Tables 5-10, 15-20)
â”‚   â”œâ”€â”€ visualizations.py            # Plotting utilities (heatmaps, comparisons)
â”‚   â”œâ”€â”€ boston_hmda.py               # Boston HMDA experiment orchestrator
â”‚   â”œâ”€â”€ german_credit.py             # German Credit experiment orchestrator
â”‚   â”œâ”€â”€ run_boston_hmda.py           # CLI runner for Boston case study
â”‚   â”œâ”€â”€ run_german_credit.py         # CLI runner for German Credit
â”‚   â””â”€â”€ run_all_case_studies.py      # Sequential runner for all case studies
â””â”€â”€ examples/
    â”œâ”€â”€ example_dvi.py               # DVI demonstration
    â”œâ”€â”€ example_svi.py               # SVI and fairness auditing
    â”œâ”€â”€ example_comparison.py        # Benchmark vs sklearn
    â””â”€â”€ example_benchmark.py         # Run synthetic benchmarks

notebooks/
â”œâ”€â”€ README.md                        # Notebook documentation
â””â”€â”€ case_studies_tutorial.ipynb     # Interactive case studies tutorial

data/                                # Dataset storage (create manually)
â”œâ”€â”€ boston_hmda/
â”‚   â””â”€â”€ hmda_data.csv               # Boston HMDA dataset
â””â”€â”€ german_credit/
    â””â”€â”€ german_credit.csv           # German Credit dataset

results/                             # Generated outputs (auto-created)
â”œâ”€â”€ boston_hmda/
â”‚   â”œâ”€â”€ tables/                     # Tables 5-7, 15-16, 19
â”‚   â””â”€â”€ figures/                    # Figure 1 (correlation heatmap)
â””â”€â”€ german_credit/
    â”œâ”€â”€ tables/                     # Tables 8-10, 17-18, 20
    â””â”€â”€ figures/                    # Figure 2 (correlation heatmap)
```

## ğŸ“– API Reference

### DirectVariableImportance

**Parameters**:
- `permutation_type` (str): `'optimal'` or `'approximate'` (default: `'optimal'`)
- `scoring_metric` (str): `'mae'`, `'mse'`, or `'rmse'` (default: `'mae'`)

**Methods**:
- `fit(model, X, y=None)`: Compute importance scores
- `get_top_features(n=5)`: Get top n important features
- `get_feature_importance(feature_idx)`: Get score for specific feature

**Attributes**:
- `importances_`: Normalized importance scores (sum to 1)
- `n_features_`: Number of features
- `feature_names_`: Feature names (if provided)

### SystemicVariableImportance

Inherits from `DirectVariableImportance` with additional parameters:

**Parameters**:
- `correlation_method` (str): `'spearman'` or `'pearson'` (default: `'spearman'`)
- `alpha` (float): Significance level for threshold (default: `0.01`)

**Methods**:
- `fit(model, X, y=None, return_decomposition=True)`: Returns (systemic, direct, indirect)
- `get_correlation_network(threshold=None)`: Get binary adjacency matrix
- `get_feature_proxies(feature_idx)`: Find proxy features

**Attributes**:
- `systemic_importances_`: Systemic scores
- `direct_importances_`: Direct component
- `indirect_importances_`: Indirect component (network effect)
- `correlation_matrix_`: Feature correlation matrix
- `correlation_threshold_`: Estimated Ï„ for significance
- `significant_correlations_`: Binary matrix of significant correlations

## ğŸ”¬ Performance Benchmarks

Based on the paper's experiments across 192 scenarios:

| Metric | DVI (Optimal) | DVI (Approximate) | Sklearn (B=1) | Sklearn (B=10) |
|--------|---------------|-------------------|---------------|----------------|
| **Speed** | âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡ | âš¡âš¡ | âš¡ |
| **Correlation with ground truth** | >0.99 | >0.99 | >0.94 | >0.94 |
| **Variance** | 0 (deterministic) | 0 (deterministic) | High | Medium |
| **Speedup vs Sklearn (B=10)** | ~20Ã— | ~100Ã— | ~10Ã— | 1Ã— |

## ğŸ“š Citation

If you use this implementation, please cite the original paper:

```bibtex
@article{dorador2025one,
  title={One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing},
  author={Dorador, Albert},
  journal={arXiv preprint arXiv:2512.13892},
  year={2025}
}
```

## ğŸ“ Educational Purpose

This implementation is designed for learning and understanding the paper's concepts:

- **Clean, readable code** with extensive docstrings
- **Simple examples** using synthetic data
- **Comments** explaining key algorithmic steps
- **Follows the paper** closely for easy cross-reference

## ğŸ” Implementation Status

This educational implementation includes:

- âœ… Core DVI and SVI algorithms (Section 3)
- âœ… Both optimal and approximate permutations (Proposition 1)
- âœ… Simple synthetic examples
- âœ… **Full 192-scenario benchmark suite** (Section 3.2)
- âœ… **Friedman function generator** (nonlinear benchmark)
- âœ… **Visualization utilities** (correlation heatmaps, importance plots, speedup comparisons)
- âœ… **Reporting functions** (tables matching paper format)
- âœ… **Boston HMDA case study** (Section 5.2) - Complete with all tables, figures, SVI scenarios
- âœ… **German Credit case study** (Section 5.3) - Complete with all tables, figures, SVI scenarios
- âœ… **Interactive Jupyter notebooks** - Tutorial covering both case studies
- âœ… **Model factory** - TRUST/RelaxedLasso, sparse logistic, random forest
- âœ… **CV utilities** - 10-fold cross-validation with paper protocol
- âœ… **Fairness testing** - Systemic importance decomposition and proxy detection
- âŒ Optional prescreening feature (omitted for clarity)

## ğŸ› ï¸ Technical Details

### Permutation Strategies

**Optimal (Rank-shift)**:
```python
def optimal_permutation(x):
    ranks = np.argsort(np.argsort(x))  # Get ranks
    n = len(x)
    shift = n // 2
    shifted_ranks = (ranks + shift) % n
    sorted_x = np.sort(x)
    return sorted_x[shifted_ranks]
```

**Approximate (Index-shift)**:
```python
def approximate_permutation(x):
    n = len(x)
    shift = n // 2
    return np.roll(x, shift)
```

### Correlation Threshold Estimation

```python
# Permute columns independently (null hypothesis: independence)
X_permuted = independently_permute_columns(X)

# Compute all pairwise correlations
correlations = compute_all_pairwise(X_permuted)

# Take (1-Î±) quantile
threshold = np.quantile(abs(correlations), 1 - alpha)
```

## âš¡ Performance Tips

1. **Use approximate permutation** for very large datasets (>100K samples)
2. **Use MAE scoring** (paper recommendation) unless you need MSE compatibility
3. **Adjust alpha** in SVI based on desired FWER control
4. **Use Spearman correlation** (handles non-linear monotonic relationships)

## ğŸ¤ Contributing

This is an educational implementation. Contributions welcome:

- Bug fixes
- Documentation improvements
- Additional examples
- Performance optimizations

## ğŸ“„ License

This implementation is provided for educational purposes. The original paper and method are by Albert Dorador.

## ğŸ™ Acknowledgments

- **Albert Dorador** for the original paper and method
- **Leo Breiman** for the foundational permutation importance concept
- **scikit-learn** community for the baseline implementation

## ğŸ“§ Contact

For questions about the implementation, please open an issue on the repository.

For questions about the paper/method, contact the paper's author.

---

**Ready to get started?**

```bash
# 1. Try basic examples with synthetic data
uv run python permutation_vi/examples/example_dvi.py
uv run python permutation_vi/examples/example_svi.py
uv run python permutation_vi/examples/example_comparison.py

# 2. Explore interactively with Jupyter
jupyter notebook notebooks/case_studies_tutorial.ipynb

# 3. Reproduce full paper results (requires datasets)
uv run python permutation_vi/case_studies/run_all_case_studies.py
```

**Next steps**:
1. ğŸ“– Read [CLAUDE.md](.claude/CLAUDE.md) for architecture and implementation details
2. ğŸ““ Explore [notebooks/README.md](notebooks/README.md) for interactive tutorials
3. ğŸ“Š Download datasets and reproduce Tables 5-10 and Figures 1-2
4. ğŸ§ª Run the full 192-scenario benchmark suite
5. ğŸ”¬ Apply DVI/SVI to your own datasets!

Happy learning! ğŸ“
